{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa_VO6HvZu22",
        "outputId": "28b7fe84-0586-4246-ee59-2fc87de315cd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "nltk.download('punkt')  # Cần tải dữ liệu cho tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import regex as re\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4fgy0tnuuWw"
      },
      "outputs": [],
      "source": [
        "with open('intents.json','r') as file:\n",
        "    intents = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHty8-9PtzSV"
      },
      "outputs": [],
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "\n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "def text_process(text):\n",
        "    text = convert_unicode(text)\n",
        "    text = text.lower()\n",
        "    # text = re.sub(r'[^swáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',text)\n",
        "    # text = re.sub(r's+', ' ', text).strip()\n",
        "    return text\n",
        "def sen_to_vec(tokenized_sen,all_words):\n",
        "    vec = np.zeros(len(all_words),dtype=np.float32)\n",
        "    for index,w in enumerate(all_words):\n",
        "        if w in tokenized_sen:\n",
        "            vec[index] = 1.0\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzeDiBAoq9eJ"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "tags = []\n",
        "pat_tag = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        pattern = text_process(pattern)\n",
        "        w = word_tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        pat_tag.append((w,tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuSc36Q2h6EN"
      },
      "outputs": [],
      "source": [
        "ignore_char = ['?','!','.',',',\"ad\", \"ạ\", \"ơi\", \"cho\", \"hỏi\", \"về\", \"làm\", \"thế\", \"nào\", \"có\",\n",
        "    \"của\", \"cho\", \"ở\", \"và\", \"có\", \"không\", \"này\", \"khi\", \"để\", \"đi\",\n",
        "    \"em\", \"mình\", \"bạn\", \"có\", \"gì\", \"nhé\", \"được\", \"ra\", \"sao\", \"mấy\",\n",
        "    \"đó\", \"người\", \"như\", \"từ\", \"là\", \"cách\", \"nhưng\", \"theo\", \"hay\",\n",
        "    \"với\", \"nên\", \"sẽ\", \"những\", \"làm\", \"nói\", \"được\", \"nếu\", \"đã\"]\n",
        "bag_word = [word for word in all_words if word not in ignore_char]\n",
        "bag_word = sorted(set(bag_word))\n",
        "tags = sorted(set(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCWcZWbfjrIq"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for (pattern_sen,tag) in pat_tag:\n",
        "    vec = sen_to_vec(pattern_sen,bag_word)\n",
        "    X_train.append(vec)\n",
        "\n",
        "    label = tags.index(tag)\n",
        "    y_train.append([label])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train, len(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLsgPIDBnoZl",
        "outputId": "24a2b6a6-4890-42b7-95bb-3e41cb82f5b8"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(len(bag_word), input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(tags), activation='softmax'))\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=5, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1roXOVMalCA",
        "outputId": "07b90b78-5973-46f3-f26c-96a5c68fee62"
      },
      "outputs": [],
      "source": [
        "with open('state_vector.json','r') as file:\n",
        "    states = json.load(file)\n",
        "\n",
        "\n",
        "all_words_state = []\n",
        "state_tags = []\n",
        "pat_tag = []\n",
        "for state in states['state_vector']:\n",
        "    state_tag = state['tag']\n",
        "    state_tags.append(state_tag)\n",
        "    for pattern in state['patterns']:\n",
        "        pattern = text_process(pattern)\n",
        "        w = word_tokenize(pattern)\n",
        "        all_words_state.extend(w)\n",
        "        pat_tag.append((w,state_tag))\n",
        "\n",
        "\n",
        "ignore_char = ['?','!','.',',','học']\n",
        "bag_word_state = [word for word in all_words_state if word not in ignore_char]\n",
        "bag_word_state = sorted(set(bag_word_state))\n",
        "state_tags = sorted(set(state_tags))\n",
        "\n",
        "\n",
        "X_train_state = []\n",
        "y_train_state = []\n",
        "for (pattern_sen,tag) in pat_tag:\n",
        "    vec = sen_to_vec(pattern_sen,bag_word_state)\n",
        "    X_train_state.append(vec)\n",
        "    label = state_tags.index(tag)\n",
        "    y_train_state.append([label])\n",
        "X_train_state = np.array(X_train_state)\n",
        "y_train_state = np.array(y_train_state)\n",
        "y_train_state = to_categorical(y_train_state, len(state_tags))\n",
        "\n",
        "\n",
        "model_state= Sequential()\n",
        "model_state.add(Dense(len(bag_word_state), input_shape=(len(X_train_state[0]),), activation='relu'))\n",
        "model_state.add(Dropout(0.5))\n",
        "model_state.add(Dense(16, activation='relu'))\n",
        "model_state.add(Dropout(0.5))\n",
        "model_state.add(Dense(len(state_tags), activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model_state.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model_state.fit(X_train_state, y_train_state, epochs=200, batch_size=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gueqIutEhJwy"
      },
      "outputs": [],
      "source": [
        "def get_pred_intent(chat,bag_word):\n",
        "    chat = text_process(chat)\n",
        "    w = np.array(word_tokenize(chat))\n",
        "    vec = sen_to_vec(w,bag_word).reshape(1,-1)\n",
        "    pred = model.predict(vec)\n",
        "    prob = pred[0, pred.argmax(axis=-1)[0]]\n",
        "    tag = tags[pred.argmax(axis=-1)[0]]\n",
        "    return prob,tag\n",
        "def get_pred_state(chat,bag_word):\n",
        "    chat = text_process(chat)\n",
        "    w = np.array(word_tokenize(chat))\n",
        "    vec = sen_to_vec(w,bag_word).reshape(1,-1)\n",
        "    pred = model_state.predict(vec)\n",
        "    prob_state = pred[0, pred.argmax(axis=-1)[0]]\n",
        "    state = state_tags[pred.argmax(axis=-1)[0]]\n",
        "    return prob_state, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6fYAdF3lb1Y"
      },
      "outputs": [],
      "source": [
        "with open('database.json','r') as file:\n",
        "    db = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nzzOdSFtccYi",
        "outputId": "a6d7ed5a-38a1-4a46-be07-cc84c179c391"
      },
      "outputs": [],
      "source": [
        "db['tuition']['Bigdata & Machinelearning']\n",
        "db['std_point']['Bigdata & Machinelearning']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk_ajORxe9uE",
        "outputId": "bd568eed-1997-48f7-8e74-cf424a00cc83"
      },
      "outputs": [],
      "source": [
        "bot_name = 'Bot'\n",
        "print(\"-------Bắt đầu-------\\n\") Xin chào, mình là chatbot ảo có thể cung cấp cho bạn các thông tin \\n liên quan đến tuyển sinh đại học như là: học phí, điểm chuẩn,... tại trường đại học Duy Tân\"\n",
        "print(f\"{bot_name}:)\n",
        "slots =[]\n",
        "while 1:\n",
        "\n",
        "    chat = text_process(input(\"User: \"))\n",
        "    if chat == \"quit\":\n",
        "        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "        break\n",
        "\n",
        "    prob_intent, tag = get_pred_intent(chat,bag_word)\n",
        "    print(\"1.\",prob_intent,tag)\n",
        "    prob_state, state = get_pred_state(chat,bag_word_state)\n",
        "    print(\"2.\",prob_state,state)\n",
        "    if prob_state > 0.85:\n",
        "        slots.clear()\n",
        "        slots.insert(1,state)\n",
        "    if tag != \"unknown\" and prob_intent > 0.85:\n",
        "        slots.insert(0,tag)\n",
        "    if slots[0] in ['tuition','std_point'] or slots[0] in state_tags:\n",
        "        while (len(slots)!=2):\n",
        "            if slots[0] in state_tags:#thieu intent\n",
        "                while 1:\n",
        "                    print(f\"{bot_name}: Bạn cần hỏi thông tin nào của ngành {slots[0]}?\")\n",
        "                    chat = text_process(input(\"User: \"))\n",
        "                    if chat == \"quit\":\n",
        "                        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "                        exit(0)\n",
        "                    prob_intent, tag = get_pred_intent(chat,bag_word)\n",
        "                    print(\"1.\",prob_intent, tag)\n",
        "                    if prob_intent > 0.85:\n",
        "                        slots.insert(0,tag)\n",
        "                        break\n",
        "            else:\n",
        "                while 1: #thieu state doi tuong\n",
        "                    print(f\"{bot_name}: Bạn cần hỏi thông tin này của ngành nào?\")\n",
        "                    chat = text_process(input(\"User: \"))\n",
        "                    if chat == \"quit\":\n",
        "                        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "                        exit(0)\n",
        "                    prob_state, state = get_pred_state(chat,bag_word_state)\n",
        "                    print(\"2.\",prob_state, state)\n",
        "                    if prob_state > 0.85:\n",
        "                        slots.insert(1,state)\n",
        "                        break\n",
        "        if slots[0] in ['tuition','std_point']:\n",
        "            for intent in intents[\"intents\"]:\n",
        "                if tag == intent['tag']:\n",
        "                    res = intent['responses'][0].format(slots[1],db[slots[0]][slots[1]])\n",
        "                    print(f\"{bot_name}: {res}\")\n",
        "                    break;\n",
        "        else:\n",
        "            for intent in intents[\"intents\"]:\n",
        "                if tag == intent['tag']:\n",
        "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "                    break;\n",
        "        slots.pop(0)\n",
        "    else:\n",
        "        for intent in intents[\"intents\"]:\n",
        "            if tag == intent['tag']:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "                break;\n",
        "        slots.clear()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
